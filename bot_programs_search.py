# –ü–æ–¥–±–æ—Ä –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã
from telebot import types
import re
from sentence_transformers import SentenceTransformer
import os
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchAny
import pymorphy2
from db import connect_db

def program_tasks(bot, show_main_menu):
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    model = SentenceTransformer('intfloat/multilingual-e5-base')
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î Qdrant
    QDRANT_URL = os.getenv("QDRANT_URL")      
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ chat_id
    user_state = {}
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    morph = pymorphy2.MorphAnalyzer()
    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    def lemmatize_words(words: list[str]) -> set[str]:
        return set(morph.parse(word)[0].normal_form for word in words if word.isalpha())
    
    def search_educational_programs(
        query_text: str,
        top_k = 10,
        min_score = 0.5,
        keyword_boost: bool = True,
        filter_keywords = None,
        final_score_threshold = 0.7
    ) -> list[dict]:
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query_words = re.findall(r'\w+', query_text.lower())
        query_keywords = lemmatize_words(query_words)
    
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        normalized_filter_keywords = [kw.lower() for kw in filter_keywords] if filter_keywords else None
        query_embedding = model.encode(query_text).tolist()
    
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã —Ñ–∏–ª—å—Ç—Ä—ã ‚Äî –¥–æ–±–∞–≤–∏–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –Ω–∏–º
        query_filter = None
        if normalized_filter_keywords:
            query_filter = Filter(
                must=[
                    FieldCondition(
                        key="metadata.keywords_lower",
                        match=MatchAny(any=normalized_filter_keywords)
                    )
                ]
            )
    
        results = client.search(
            collection_name="abit-itmo-programs",
            query_vector=query_embedding,
            query_filter=query_filter,
            limit=top_k * 5,
            with_payload=True,
            score_threshold=min_score
        )
    
        # Boost –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –Ω–∞–∑–≤–∞–Ω–∏—é, –æ–ø–∏—Å–∞–Ω–∏—é
        if keyword_boost and results:
            boosted_results = []
            for hit in results:
                payload = hit.payload
                keywords = lemmatize_words(payload["metadata"].get("keywords", []))
                title = lemmatize_words(re.findall(r'\w+', payload["–Ω–∞–∑–≤–∞–Ω–∏–µ"].lower()))
                description = lemmatize_words(re.findall(r'\w+', payload["–æ–ø–∏—Å–∞–Ω–∏–µ"].lower()))
    
                # –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                match_keywords = len(query_keywords & keywords)
                match_title = len(query_keywords & title)
                match_desc = len(query_keywords & description)
    
                # –í—ã—á–∏—Å–ª—è–µ–º boost
                boost = 1.0
                boost += 0.25 * match_keywords
                boost += 0.5 * match_title
                boost += 0.1 * match_desc
    
                # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ –ø–æ–ª–µ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç ‚Äî —à—Ç—Ä–∞—Ñ
                if match_keywords == 0 and match_title == 0:
                    boost *= 0.6
    
                hit.score *= boost
                boosted_results.append(hit)
    
            results = [
                hit for hit in boosted_results if hit.score >= final_score_threshold
            ]
            results.sort(key=lambda x: x.score, reverse=True)
            results = results[:top_k]
    
        return [
            {
                "title": hit.payload["–Ω–∞–∑–≤–∞–Ω–∏–µ"],
                "match_score": round(hit.score, 3),
                "description": hit.payload["–æ–ø–∏—Å–∞–Ω–∏–µ"],
                "link": hit.payload["metadata"]["—Å—Å—ã–ª–∫–∞"]
            }
            for hit in results[:3]
        ]
    
    def escape_markdown_v2(text):
        if not text:
            return ''
        escape_chars = r'\_*[]()~`>#+-=|{}.!'
        return re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text)
    
    # === –•–µ–Ω–¥–ª–µ—Ä—ã ===
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã
    @bot.message_handler(func=lambda message: message.text == "–í—ã–±—Ä–∞—Ç—å –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—É")
    def ask_interest(message):
        bot.send_message(
            message.chat.id,
            "–†–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ –±—ã —Ç—ã —Ö–æ—Ç–µ–ª –∏–∑—É—á–∞—Ç—å ‚Äî –∏ —è –ø–æ–¥–±–µ—Ä—É –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã"
        )
        user_state[message.chat.id] = 'waiting_for_interest'
    # –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é
    @bot.message_handler(func=lambda message: message.text == "–í–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é")
    def back_to_main_menu(message):
        chat_id = message.chat.id
        if chat_id in user_state:
            del user_state[chat_id]
        show_main_menu(chat_id)
    # –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –≤—ã–±–æ—Ä –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã
    @bot.message_handler(func=lambda message: message.text == "–í—ã–±—Ä–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É —Å–Ω–æ–≤–∞")
    def repeat_program_selection(message):
        chat_id = message.chat.id
        bot.send_message(chat_id, "–†–∞—Å—Å–∫–∞–∂–∏, –∫–∞–∫—É—é –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—É —Ç—ã –∏—â–µ—à—å")
        user_state[chat_id] = 'waiting_for_interest'
    # –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–æ 3 –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º
    @bot.message_handler(func=lambda message: user_state.get(message.chat.id) == 'waiting_for_interest' or (
        isinstance(user_state.get(message.chat.id), dict) and user_state.get(message.chat.id, {}).get('step') == 'waiting_for_interest'))
    def handle_interest(message):
        query = message.text
        matches = search_educational_programs(query)
    
        if matches:
            response = "–í–æ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã:\n\n"
            for prog in matches:
                title = escape_markdown_v2(prog.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'))
                description = escape_markdown_v2(prog.get('description', ''))
                link = escape_markdown_v2(prog.get('link', ''))
                response += f"üéì *{title}*\n{description}\n{link}\n"
    
            bot.send_message(
                message.chat.id,
                response.strip(),
                parse_mode='MarkdownV2'
            )
            user_state[message.chat.id] = {
                'last_matches': matches,
                'step': 'after_search'
            }
    
            markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)
            markup.add("–£–∑–Ω–∞—Ç—å –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ")
            markup.add("–í—ã–±—Ä–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É —Å–Ω–æ–≤–∞", "–í–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é")
    
            bot.send_message(
            message.chat.id,
            "–•–æ—á–µ—à—å —É–∑–Ω–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ–± –æ–¥–Ω–æ–π –∏–∑ —ç—Ç–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º?\n–ù–∞–∂–º–∏ <b>–£–∑–Ω–∞—Ç—å –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ</b>",
            reply_markup=markup, parse_mode="HTML")
        else:
            markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)
            markup.add("–í—ã–±—Ä–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É —Å–Ω–æ–≤–∞", "–í–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é")
            bot.send_message(
                message.chat.id,
                "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–æ—Å–µ—Ç–∏ –Ω–∞—à —Å–∞–π—Ç: https://abit.itmo.ru/master",
                reply_markup=markup
            )
            bot.send_sticker(message.chat.id, "CAACAgIAAxkBAAIGdWg0T-k_wg6GxgTNmd6qm9LDkyPMAALidwACREyZSaa6TlUTzrRcNgQ")
